{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IUvRApnRKSI9"
      },
      "source": [
        "NOMBRES: Mariel Alejandra \n",
        "\n",
        "APELLIDOS: Guamuche Recinos\n",
        "\n",
        "CARNE: 21150\n",
        "\n",
        "FECHA: 06/08/2025"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-7-RdQDQsYE"
      },
      "source": [
        "**Instrucciones:**\n",
        "\n",
        "El objetivo de esta práctica es que apliquen lo visto en clase. Al texto \"metamorphosis_kafka\" que pueden encontrarlo en el siguiente link: https://www.gutenberg.org/ebooks/5200 (Usar Plain Text)\n",
        "deben aplicarse técnicas de tokenización, lemmatización, stemming y normalización. Sin embargo, estos procesos no se aplican de manera indiscriminada. Para este proyecto tienen que aplicar las técnicas a **1 de 2 objetivos**.\n",
        "\n",
        "1. Este texto se usará para entrenar un LLM para que escriba con un estilo kafkiano.\n",
        "2. Reconocer textos escritor por Kafka. El objetivo final en esta línea sería poder etiquetar algo como \"kafka\" y \"no kafka\".\n",
        "\n",
        "Para ambas perspectivas qué tipos de tokenización, lemmatización, stemming y normalización utilizaría. Ej: tokenizaría por oraciones en lugares de palabras, tomaría o no en cuenta signos de puntuación, removería stopwords o no, etc. Si para esto consulta otras fuentes, citelas.\n",
        "\n",
        "Luego de explicar ambas perspectivas **escoja 1** y programe cada una de las etapas.\n",
        "\n",
        "Se recomienda que antes de empezar, revise el texto para ver si hay cosas que le conviene editar antes de empezar a programar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Características del texto:\n",
        "- Se evidencian oraciones extensas, con varios signos de puntuación.\n",
        "- Los diálogos entre personajes están señalados entre comillas dobles.\n",
        "- En la traducción del texto proporcionado, se utilizan gran variedad de contracciones en inglés \"don't\"->\"do not\".\n",
        "- De acuerdo a muchos que describen el estilo kafkiano, se trata de textos que abarcan la temática existencial, hasta cierto punto fatalistas. Además de la represión del individuo y en análisis instrospectivo, que puede confundir al lector. \n",
        "\n",
        "*El texto proporcionado incluye al principio y al final información del libro, datos que fueron eliminados manualmente*\n",
        "## Pipeline para el segundo modelo: clasificación de textos\n",
        "### Descripción del pipeline a implementar\n",
        "1. Normalización\n",
        "Se utiliza normalización unicode y lowercasing con el fin de unificar diferentes codificaciones y evitar crear tokens con mayúsuculas y minúsculas. \n",
        "Adicionalmente, se ha implementado la expansión de contracciones para evitar que el modelo trate a \"don't\" como token distinto de \"do not\". Esto se espera mejora la claridad semántica.\n",
        "\n",
        "2. Eliminación de puntuación y caracteres no alfabéticos\n",
        "Usando expresiones regulares se elimina la puntuación que no aporta significado semántico al modelo de clasificación y genera tokens irrelevantes. A pesar que las estructuras de uso \"exagerado\" de puntuación en los textos de Kafka, esto podría confundir al modelo donde expresiones como \"¡¡¡Feliz cumpleaños!!! eres la mejor...\" podría clasificarlos como de Kafka.\n",
        "\n",
        "3. Tokenización\n",
        "Tokenización por palabras para operar sobre cada token los demás pasos del pipeline. Se considero hacer separación por oraciones para tratar de tener features por oraciones en la escritura de Kafka, pero esto implicaría un alto consumo de memoria y el modelo no aprendería (posiblemente) las generalidades de la redacción de Kafka. Donde además se reduciría el vocabulario.\n",
        "\n",
        "4. Eliminación de stopwords\n",
        "\n",
        "5. Stemming \n",
        "Usando PorterStemmer se reduce variantes para compactar el vocabulario. Se utiliza Porter Stemmer porque el texto está en inglés y permite agrupar palabras similares.\n",
        "Algunas páginas recomiendan utilizar más stemming que lemmatización si se trata de un modelo de clasificación.\n",
        "https://kazumatsuda.medium.com/nlp-preprocessing-text-data-part-1-b4641af2a5af\n",
        "https://mylearningsinaiml.wordpress.com/nlp/data-preparation/clean-text/ \n",
        "\n",
        "6. Lematización \n",
        "Usando el etiqueta POS básico, se aplica lematización para mapear cada término a su forma canónica y es opcional para el código.\n",
        "\n",
        "Para la implementación, se ha puesto un criterio de si se desea manejar con lematización o stemming o ambas.\n",
        "\n",
        "7. Reconstrucción\n",
        "Opción para unir los tokens en un string o dejarlos coom lista de palabras.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Implementación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Librería para expandir las contracciones\n",
        "# !pip install contractions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to C:\\Users\\Mariel\n",
            "[nltk_data]     Guamuche\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to C:\\Users\\Mariel\n",
            "[nltk_data]     Guamuche\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to C:\\Users\\Mariel\n",
            "[nltk_data]     Guamuche\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Mariel Guamuche\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package omw-1.4 to C:\\Users\\Mariel\n",
            "[nltk_data]     Guamuche\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "import unicodedata\n",
        "import nltk\n",
        "import contractions\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('omw-1.4')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
        "from nltk import pos_tag, word_tokenize\n",
        "from nltk.corpus.reader.wordnet import NOUN, VERB, ADJ, ADV\n",
        "\n",
        "# Mapeo de etiquetas POS de nltk.pos_tag a wordnet\n",
        "POS_MAP = {\n",
        "    'J': ADJ,\n",
        "    'V': VERB,\n",
        "    'N': NOUN,\n",
        "    'R': ADV\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {},
      "outputs": [],
      "source": [
        "def normalize_text(text: str) -> str:\n",
        "    nkfd = unicodedata.normalize('NFKD', text)\n",
        "    no_accents = ''.join(c for c in nkfd if not unicodedata.combining(c))\n",
        "    return no_accents.lower()\n",
        "\n",
        "def expand_contractions(text: str) -> str:\n",
        "    # usa la librería contractions para convertir contracciones en su forma completa\n",
        "    return contractions.fix(text)\n",
        "\n",
        "def remove_punctuation(text: str) -> str:\n",
        "    return re.sub(r'[^a-z\\s]', ' ', text)\n",
        "\n",
        "def tokenize(text: str) -> list[str]:\n",
        "    return word_tokenize(text)\n",
        "\n",
        "def remove_stopwords(tokens: list[str]) -> list[str]:\n",
        "    stops = set(stopwords.words('english'))\n",
        "    return [t for t in tokens if t not in stops and t.isalpha()]\n",
        "\n",
        "def stem_tokens(tokens: list[str]) -> list[str]:\n",
        "    stemmer = PorterStemmer()\n",
        "    return [stemmer.stem(t) for t in tokens]\n",
        "\n",
        "def lemmatize_tokens(tokens: list[str]) -> list[str]:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    lemmata = []\n",
        "    for token, tag in pos_tags:\n",
        "        pos = POS_MAP.get(tag[0], NOUN)\n",
        "        lemmata.append(lemmatizer.lemmatize(token, pos=pos))\n",
        "    return lemmata\n",
        "\n",
        "def preprocess_pipeline(text: str,\n",
        "                        do_stem: bool = True,\n",
        "                        do_lemmatize: bool = True) -> list[str]:\n",
        "    \"\"\"\n",
        "    Pipeline completo:\n",
        "    1. Normalizar y pasar a minúsculas\n",
        "    1.5 Expandir contracciones\n",
        "    2. Quitar puntuación\n",
        "    3. Tokenizar\n",
        "    4. Eliminar stopwords\n",
        "    5. Stemming (opcional)\n",
        "    6. Lematización (opcional)\n",
        "    \"\"\"\n",
        "    # 1. Normalizar\n",
        "    t = normalize_text(text)\n",
        "    # 1.5 Expandir contracciones\n",
        "    t = expand_contractions(t)\n",
        "    # 2. Quitar puntuación\n",
        "    t = remove_punctuation(t)\n",
        "    # 3. Tokenizar\n",
        "    tokens = tokenize(t)\n",
        "    # 4. Eliminar stopwords y no-alfabéticos\n",
        "    tokens = remove_stopwords(tokens)\n",
        "    # 5. Stemming\n",
        "    if do_stem:\n",
        "        tokens = stem_tokens(tokens)\n",
        "    # 6. Lematización\n",
        "    if do_lemmatize:\n",
        "        tokens = lemmatize_tokens(tokens)\n",
        "    return tokens"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Ejemplo de uso:\n",
        "with open('pg5200.txt', 'r', encoding='utf-8') as f:\n",
        "    sample = f.read()\n",
        "tokens = preprocess_pipeline(sample)\n",
        "import collections\n",
        "cnt = collections.Counter(tokens)\n",
        "# Calculate stylistic features\n",
        "features = {\n",
        "    'num_tokens': len(tokens),\n",
        "    'num_unique_tokens': len(cnt),\n",
        "    'vocab_size': len(cnt),\n",
        "    'most_common': cnt.most_common(20),\n",
        "    'avg_tokens_per_text': float(np.mean([len(doc) for doc in [tokens]]))\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'num_tokens': 10035,\n",
              " 'num_unique_tokens': 1838,\n",
              " 'vocab_size': 1838,\n",
              " 'most_common': [('gregor', 298),\n",
              "  ('would', 204),\n",
              "  ('room', 133),\n",
              "  ('could', 127),\n",
              "  ('even', 102),\n",
              "  ('father', 102),\n",
              "  ('sister', 101),\n",
              "  ('door', 97),\n",
              "  ('mother', 90),\n",
              "  ('go', 86),\n",
              "  ('back', 83),\n",
              "  ('make', 76),\n",
              "  ('time', 74),\n",
              "  ('one', 73),\n",
              "  ('say', 73),\n",
              "  ('get', 71),\n",
              "  ('come', 69),\n",
              "  ('way', 63),\n",
              "  ('look', 61),\n",
              "  ('take', 55)],\n",
              " 'avg_tokens_per_text': 10035.0}"
            ]
          },
          "execution_count": 39,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "features"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
